{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea2d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "737588bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/gxu21/fair-fairytale-nlp/booknlp'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942e2d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(__package__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e087dfd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14789/2030855153.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbooknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbooknlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBookNLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from .booknlp.booknlp import BookNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5395293c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda\n",
      "{'pipeline': 'coref,entity,quote', 'model': 'big'}\n",
      "--- startup: 19.820 seconds ---\n",
      "--- spacy: 0.197 seconds ---\n",
      "--- entities: 1.390 seconds ---\n",
      "--- quotes: 0.004 seconds ---\n",
      "--- attribution: 0.480 seconds ---\n",
      "--- name coref: 0.002 seconds ---\n",
      "--- coref: 3.369 seconds ---\n",
      "***********printing variables\n",
      "--- TOTAL (excl. startup): 5.603 seconds ---, 1968 words\n"
     ]
    }
   ],
   "source": [
    "from ./booknlp.booknlp import BookNLP\n",
    "import os\n",
    "\n",
    "model_params={\n",
    "    \"pipeline\":\n",
    "    \"coref,entity,quote\"\n",
    "    #, supersense,event,\n",
    "        ,\"model\":\"big\"\n",
    "    #,\"pronominalCorefOnly\":False\n",
    "    }\n",
    "\n",
    "booknlp=BookNLP(\"en\", model_params)\n",
    "\n",
    "\n",
    "# Input file to process\n",
    "input_file= os.path.join(\"../FairytaleQA_Dataset/split_by_origin\",\"chinese-fairybook/old-dschang-story.csv\")\n",
    "# Output directory to store resulting files in\n",
    "output_directory=\"testing_output/\"\n",
    "\n",
    "# File within this directory will be named ${book_id}.entities, ${book_id}.tokens, etc.\n",
    "name = \"\".join(model_params[\"pipeline\"])\n",
    "book_id= input_file.split(\".\")[0] +\"_\"+ name \n",
    "book_id= \"old-dschang-story\"+\"_\"+ name \n",
    "\n",
    "booknlp.process(input_file, output_directory, book_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b030a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = \"./testing_output/old-dschang-story_coref,entity,quote.entities\"\n",
    "df = pd.read_csv(path, sep=\"\t\")\n",
    "df\n",
    "\n",
    "path2 = \"./testing_output/old-dschang-story_coref,entity,quote.character_meta\"\n",
    "df1 = pd.read_csv(path2, sep=\"\t\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe8ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_name = list(df1[\"clustered_names\"])\n",
    "name_dict = dict()\n",
    "for i in range(df.shape[0]):\n",
    "    line = df.iloc[i]\n",
    "    coref,start,end,prop,cat,text = \\\n",
    "        line[\"COREF\"],line[\"start_token\"],line[\"end_token\"],line[\"prop\"],line[\"cat\"],line[\"text\"]\n",
    "        #when the previous token is located within the current token\n",
    "    if i>0 and int(df.iloc[i-1][\"start_token\"]) >= int(start) and int(df.iloc[i-1][\"end_token\"])<=int(end) :\n",
    "        #if the current token is a PER NOM, and prev a PER,PRON\n",
    "        if cat == \"PER\" and prop ==\"NOM\":\n",
    "\n",
    "            if df.iloc[i-1][\"prop\"]==\"PRON\" and df.iloc[i-1][\"cat\"]==\"PER\":\n",
    "                #if all thos conditions are satisfied, we need to make change to the name of this coref\n",
    "                \n",
    "                #if the pronoun can be coreferred to a NOM term, and it's not already there then we will update the name\n",
    "                pron_ls = df1.index[df1['coref_idx'] == df.iloc[i-1][\"COREF\"] ].tolist()\n",
    "                if len(pron_ls)==1:\n",
    "                    pron_idx = pron_ls[0]\n",
    "                    name = df1.iloc[pron_idx][\"clustered_names\"]\n",
    "                    name = easy_name[pron_idx]\n",
    "\n",
    "                    #we should definitely be able to find a colomn in df1, with this coref idx:\n",
    "                    idx = df1.index[df1['coref_idx'] == coref].tolist()[0]\n",
    "                    if (idx not in name_dict) or (idx in name_dict and df1.iloc[idx][\"clustered_names\"] not in name_dict[idx]):\n",
    "                        ex = df1.iloc[idx][\"clustered_names\"].split(\"/\")[0]\n",
    "                        #remove the first pronoun term\n",
    "                        rm_ex = \" \".join(ex.split(\" \")[1:])\n",
    "                        easy_name[idx] = name.split(\"/\")[0]+\"'s\" + \" \" + rm_ex\n",
    "                        df1.at[idx, \"clustered_names\"]= name+\"--\"+ df1.iloc[idx][\"clustered_names\"]\n",
    "                        \n",
    "                    if idx not in name_dict:\n",
    "                        name_dict[idx] = set()\n",
    "                        name_dict[idx].add(df1.iloc[idx][\"clustered_names\"])\n",
    "                    else:\n",
    "                        name_dict[idx].add(df1.iloc[idx][\"clustered_names\"])\n",
    "                        \n",
    "                        \n",
    "df1[\"easy_name\"] = easy_name\n",
    "df1 = df1.sort_values(\"total\", ascending=False)\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5326f405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#you can run the following code to run the boookNLP code on every story in the split_by_origin \n",
    "#folder of the FairytaleQA_Dataset;\n",
    "#it will create a result folder in the same directory as the story file. \n",
    "\n",
    "\n",
    "import os\n",
    "from booknlp.booknlp import BookNLP\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_params={\n",
    "    \"pipeline\":\n",
    "    \"coref,entity,quote\"\n",
    "    #, supersense,event,\n",
    "        ,\"model\":\"big\"\n",
    "    #,\"pronominalCorefOnly\":False\n",
    "    }\n",
    "\n",
    "booknlp=BookNLP(\"en\", model_params)\n",
    "\n",
    "\n",
    "path = \"../FairytaleQA_Dataset/split_by_origin\"\n",
    "origin_ls = os.listdir(path)\n",
    "for idx,origin in tqdm(enumerate(origin_ls)):\n",
    "    origin_path = os.path.join(path,origin)\n",
    "    stories = os.listdir(origin_path)\n",
    "    for story in tqdm(stories):\n",
    "        story_path = os.path.join(origin_path,story)\n",
    "        # Input file to process\n",
    "        input_file= story_path\n",
    "        \n",
    "        story_folder = story_path.split(\".\")[0]\n",
    "        \n",
    "        isExist = os.path.exists(story_folder)\n",
    "        if not isExist:\n",
    "            # Create a new directory because it does not exist \n",
    "            os.makedirs(story_folder)\n",
    "\n",
    "        # Output directory to store resulting files in\n",
    "        output_directory=story_folder\n",
    "        # File within this directory will be named ${book_id}.entities, ${book_id}.tokens, etc.\n",
    "        name = \"\".join(model_params[\"pipeline\"])\n",
    "        book_id= input_file.split(\".\")[0].split(\"/\")[-1] +\"_\"+ name\n",
    "        try:\n",
    "            booknlp.process(input_file, output_directory, book_id)\n",
    "        except:\n",
    "            print(\"failed to process one story: \", story)\n",
    "            pass\n",
    "    print(\"orgin processed: \", origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c948a68e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
